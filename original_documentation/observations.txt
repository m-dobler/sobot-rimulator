OBSERVATIONS AND HYPOTHESES TO INVESTIGATE FURTHER

Wheel Encoder Behavior:
======================
The discrete nature of the wheel encoder essentially results in a discrete set of points in the world where the robot may believe it is located. It would be interesting to figure out what the complete set of such points looks like.


GTG PID Controller:
==================
When GTG controller uses angle to goal as it's error, a relatively small integral term will induce oscillations, which may be damped by the other terms.

Using the distance to the goal as the error does not work. Is this because the state-space is non-linear in such a model?

Idea behind good unicycle velocity as function of unicycle omega (angular velocity).

AO PID Controller:
=================
If it happens that all of the sensors are reporting similar distances to obstacles, the resulting avoid-obstacle heading vector behaves similarly to when none of the sensors are detecting obstacles. Thus if the robot finds itself in a very cramped place with obstacles on all sides, it will behave as if there are no obstacles - driving forward and crashing.

It may seem reasonable to give the forward sensors a greater weighting in the computed avoidance heading. However, the fact there is also a greater number of forward-facing sensors on the robot must be taken into account, as this also gives a higher sensitivity to obstacles in front of the robot. It turns out that giving slightly HIGHER weights the further back a sensor points compensates well, and gives the robot a better chance of escaping wedges and other situations where the above note applies.

Blended GTG/AO Controller:
========================
A very heavy weight towards the avoid-obstacle heading seems to be necessary when there is even a small amount of clutter.

Raising the sensor gains on the side-facing sensors significantly improves the robot's ability to handle corners of obstacles.

Adding some nonlinear scaling to the sensor distances (implemented in the AO controller) seems to offer some subtle improvements to behavior. That is, having the robot react less aggressively to obstacles just barely within its sensor range, but very aggressively to obstacles it is almost colliding with. I am not commiting the code for this at this time but here is a good polynomial formula for scaling a sensor distance vector before adding it to the avoid-obstacle heading vector:
      # obstacle_vector = the real vector from the robot to the obstacle
      # d               = real sensor distance (length of obstacle_vector)
      # dmin, dmax      = min and max distances the sensor is capable of detecting
      # a               = scaling factor equal to 1.0 / ( dmax-dmin )
      # b               = exponent - should be between 0.0 and 1.0 to give desired behavior
      # with these definitions the scaling factor s will be 1.0 when d = dmax, falling at an accelarating rate as d decreases, reaching 0.0 when d = dmin:
      s = ( a*(d - dmin) )**b
      scaled_vector = linalg.scale( obstacle_vector, s )

FW PID Controller
=================
Determining how the robot should behave when none of the bearing sensors are detecting a wall is tricky. This can occur when the robot has to round a corner. To prevent the robot from peeling off the wall at corners, it's "nodetect" behavior must be to turn in the direction of the bearing sensors. This ensures it will pick up the wall again. This also leads us to define the previously-undefined behavior of the follow-wall controller when there are no walls to follow: it will turn constantly in a circle in the direction of its bearing sensors.

The behavior of the robot depends on the specific arrangement of the sensors. In this case it is critical that the controller bears on the rearmost sensor when one or zero sensors are in range of the wall. This is the only sensor that is greater than 90 degrees from the front of the vehicle. Defaulting to this sensor is the only way to ensure that the heading vector always points forward and towards the bearing side when one or zero sensors is detecting the wall.

It turns out that the above strategy causes miscalculations when the supervisor state machine is determining which direction to follow an encountered obstacle. This is due to the fact that the heading vectors for the side of the robot that is detecting the obstacle and the side of robot that is not detecting anything may both point towards the same side of the robot. To mitigate this confusion, when no obstacle is detected, an updated design uses carefully selected reference points that are independent of the sensors. These updated references give the same desired behavior of turning the robot towards the bearing side, but the heading vector given is less likely to confuse the switching algorithms.
